[["index.html", "Proyecto Book LCC Capítulo 1 Introducción", " Proyecto Book LCC Antonio Lara Gutiérrez y Víctor Ramírez Mármol 2022-06-07 Capítulo 1 Introducción Este book ha sido creado como parte de un proyecto de la asignatura Laboratorio de Computación Científica del Grado de Ingeniería Informática en la Universidad de Málaga. Este book contiene dos ejercicios propuestos por el profesor: Ejercicio grupal: ejercicio donde tomaremos datos procedentes de la empresa de ciberseguridad Virus Total, y a los que aplicaremos conceptos vistos durante el curso para extraer conocimiento. Este ejercicio grupal se ha relizado junto al compañero Víctor Ramírez Mármol. Se puede consultar el repositorio en GitHub con toda la información recogida del proyecto, a través de este enlace. Ejercicio individual: ejercicio donde aplicaremos textmining o minería de texto a un registro de mensajes de Whatsapp, para obtener información oculta a partir de los datos obtenidos. "],["análisis-de-datos-de-virus.html", "Capítulo 2 Análisis de datos de virus 2.1 Metodologia 2.2 Trabajar con archivos JSON 2.3 Creación del dataset", " Capítulo 2 Análisis de datos de virus En este capítulo presentaremos la información que hemos recibido para aplicar análisis de datos de parte de Virus Total, y preparemos los datasets para luego poder procesarlos de manera correcta en R. 2.1 Metodologia En este primer capítulo de pre-processing mostraremos algunos ejemplos de como trabajar con archivos de este tipo en R, realizando un mapeo que introduzca toda la información de estos en un dataset, para posteriormente poder ser manipulada. En los siguientes capítulos prepararemos una visualización de los datos, así como la aplicación de todas las técnicas de datos que hemos visto a lo largo de la asignatura: Reglas de asociación usando el paquete arules Clustering usando el algoritmo kmeans Análisis de conceptos formales usando el paquete fcaR Regresión lineal univariable y multivariable 2.2 Trabajar con archivos JSON Los datos recibidos conforman un paquete con distintos archivos JSON. Los archivos JSON pueden ser procesados en R, introduciendo el contenido que muestran en un objeto dataframe. Los archivos con los que trataremos contienen información de la forma clave:valor. Veamos un ejemplo de como mapear el siguiente JSON, usando un archivo de prueba, que contiene la siguiente información: \\({``name&quot;:``virus&quot;,``surname&quot;:``total&quot;,``date&quot;: ``2021-11-03 00:19:30&quot;}\\) Para poder leer este archivo, necesitamos de la librería tidyjson, que nos proporciona herramientas para convertir datos en JSON a data frames, que es justo lo que necesitamos. Instalaremos el paquete con la orden: install.packages(&quot;tidyjson&quot;) El archivo que contiene la información se llama test.json. Para obtener un data frame con la información de este, ejecutamos los siguientes comandos: json_data &lt;- read_json(&quot;test.json&quot;) # leemos de fichero json_data # información no mapeada ## # A tbl_json: 1 x 2 tibble with a &quot;JSON&quot; attribute ## ..JSON document.id ## &lt;chr&gt; &lt;int&gt; ## 1 &quot;{\\&quot;name\\&quot;:\\&quot;virus\\&quot;...&quot; 1 item &lt;- json_data %&gt;% spread_all() # extraemos informacion paged_table(item) 2.3 Creación del dataset Una vez que ya sabemos extraer información de un fichero JSON, vamos a proceder a leer secuencialmente todos los ficheros que nos han proporcionado, y vamos a insertar su información en un data frame. Posteriormente, guardaremos la información de este en formato .csv, al que llamaremos android.csv, para ahorrar el tener que procesar de nuevo todos los ficheros. Los archivos se encuentran dentro de la carpeta Android, y son aquellos que terminan en .json ficheros &lt;- Sys.glob(&quot;Android/*.json&quot;) length(ficheros) # Contamos con un total de 183 ficheros de datos de virus ## [1] 183 head(ficheros,10) ## [1] &quot;Android/01019ec606ad4a54daff819207042dc7772db7cd9d689eceb0a7276bc5896494.json&quot; ## [2] &quot;Android/0238cc91f4d916cf68416b5320898e9e5c7fa490d8ccdb76d2c9aa57162d95bd.json&quot; ## [3] &quot;Android/03238968f7314e8f22eef1fef6dc6ed78f0e365e2f77296d64e83ccd9c9aaac3.json&quot; ## [4] &quot;Android/03fdc546d08dadab6d83194ccd0348aaa917d375367fd514faaeb8b5d5c2c620.json&quot; ## [5] &quot;Android/040b64c4b1f02814f81e3597aae8da2295e68a2a6d574fedcdbe0fa9a90ce8d2.json&quot; ## [6] &quot;Android/06b523da0f9d2d52ddf3b2c76a8be5e4084bd921e363b36c7e3bb3c456aa5a39.json&quot; ## [7] &quot;Android/07b95c42d347677e3d875c3c788b2b326ab458d90881089d0ebe9dfacfea7da9.json&quot; ## [8] &quot;Android/07d7320c1e857c8f5e8b37b7e2054fcf71314ef9e574f1159c7b53c9530a339a.json&quot; ## [9] &quot;Android/0ae56408c62a25c828a3c304e30328c162c5382965ce1507f16543cb399bb971.json&quot; ## [10] &quot;Android/0b911143341c6fd040b785df145f64b6a0a3751d55acb0c990f6c326ae7690fa.json&quot; A continuación, vamos a iterar cada uno de ellos, a los que aplicaremos los comandos read_json para obtener toda la información y spread_all para extraerla en un data frame. Montaremos el data frame usando bind_rows first = TRUE for (file in ficheros){ json_data &lt;- read_json(file) if(first){ android &lt;- json_data %&gt;% spread_all() first = FALSE }else { android &lt;- bind_rows(android,json_data %&gt;% spread_all()) } } Con esto tendremos un dataframe con 183 filas (una para cada fichero de información) y 447 variables, que corresponde con la información que guarda cada fichero. Finalmente, vamos a escribir esta información en un archivo .csv, usando para ello el comando write.csv sobre la variable con los datos que acabamos de crear, y visualizamos su contenido android &lt;- apply(android,2,as.character) # eliminamos caracteres no deseados write.csv(android,file=&quot;android.csv&quot;) android &lt;- read_csv(&quot;android.csv&quot;) paged_table(android) # toda la información extraída "],["visualización-de-datos.html", "Capítulo 3 Visualización de datos 3.1 Filtrado de información 3.2 Visualización de los datos", " Capítulo 3 Visualización de datos En este capítulo vamos a realizar un primer análisis exploratorio sobre los atributos principales que usaremos en el próximo capítulo de análisis de datos, junto con una visualización inicial de los datos con los que contamos 3.1 Filtrado de información Partiendo del dataset procesado y guardado en el fichero android.csv, nos encontramos con la siguiente información: android &lt;- read_csv(&quot;android.csv&quot;) dim(android) # 183 objetos y 447 atributos ## [1] 183 447 head(colnames(android),30) ## [1] &quot;...1&quot; &quot;document.id&quot; ## [3] &quot;vhash&quot; &quot;scan_date&quot; ## [5] &quot;first_seen&quot; &quot;total&quot; ## [7] &quot;size&quot; &quot;authentihash&quot; ## [9] &quot;times_submitted&quot; &quot;harmless_votes&quot; ## [11] &quot;malicious_votes&quot; &quot;sha256&quot; ## [13] &quot;type&quot; &quot;scan_id&quot; ## [15] &quot;unique_sources&quot; &quot;positives&quot; ## [17] &quot;ssdeep&quot; &quot;md5&quot; ## [19] &quot;permalink&quot; &quot;sha1&quot; ## [21] &quot;response_code&quot; &quot;community_reputation&quot; ## [23] &quot;verbose_msg&quot; &quot;last_seen&quot; ## [25] &quot;additional_info.trid&quot; &quot;additional_info.magic&quot; ## [27] &quot;submission.submitter_region&quot; &quot;submission.date&quot; ## [29] &quot;submission.submitter_country&quot; &quot;submission.filename&quot; En estos primeros 30 atributos nos encontramos la información más importante. En el resto de atributos nos aparece información adicional y el resultado del escaneo con todos los antivirus que han sido probados. En este resultado nos aparece el tipo de virus que ha sido identificado por el motor del antivirus, y aparece en la columna scans.nombreAntivirus.result: android &lt;- android %&gt;% select(scan_date,first_seen,last_seen,total,size, times_submitted,harmless_votes,malicious_votes,positives, submission.submitter_country,additional_info.exiftool.FileType, additional_info.androguard.TargetSdkVersion, contains(&quot;.result&quot;)) write.csv(android,file=&quot;android-simplified.csv&quot;) Hemos filtrado toda la información importante y guarda en el fichero android-simplified.csv, que es el que usaremos finalmente en nuestro análisis de datos android &lt;- read_csv(&quot;android-simplified.csv&quot;) dim(android) # 183 objetos y 79 atributos ## [1] 183 79 android %&gt;% select(contains(&quot;.result&quot;)) %&gt;% dim() ## [1] 183 66 Vemos que hay un total de 66 antivirus distintos usados en el análisis de los archivos: #obtenemos nombre de los antivirus antivirus &lt;- colnames(android)[grepl(&quot;result$&quot;,colnames(android))] #extraemos nombres, usando stringr antivirus %&gt;% str_replace(&quot;scans.&quot;, &quot;&quot;) %&gt;% str_replace(&quot;.result&quot;, &quot;&quot;) ## [1] &quot;Bkav&quot; &quot;Lionic&quot; &quot;ClamAV&quot; ## [4] &quot;CMC&quot; &quot;CAT-QuickHeal&quot; &quot;McAfee&quot; ## [7] &quot;ALYac&quot; &quot;Malwarebytes&quot; &quot;VIPRE&quot; ## [10] &quot;Sangfor&quot; &quot;K7AntiVirus&quot; &quot;Alibaba&quot; ## [13] &quot;K7GW&quot; &quot;Trustlook&quot; &quot;Arcabit&quot; ## [16] &quot;Baidu&quot; &quot;Cyren&quot; &quot;SymantecMobileInsight&quot; ## [19] &quot;Symantec&quot; &quot;ESET-NOD32&quot; &quot;TrendMicro-HouseCall&quot; ## [22] &quot;Avast&quot; &quot;Cynet&quot; &quot;Kaspersky&quot; ## [25] &quot;BitDefender&quot; &quot;NANO-Antivirus&quot; &quot;SUPERAntiSpyware&quot; ## [28] &quot;MicroWorld-eScan&quot; &quot;Rising&quot; &quot;Ad-Aware&quot; ## [31] &quot;Comodo&quot; &quot;F-Secure&quot; &quot;DrWeb&quot; ## [34] &quot;Zillya&quot; &quot;TrendMicro&quot; &quot;McAfee-GW-Edition&quot; ## [37] &quot;FireEye&quot; &quot;Emsisoft&quot; &quot;Ikarus&quot; ## [40] &quot;Avast-Mobile&quot; &quot;Jiangmin&quot; &quot;Avira&quot; ## [43] &quot;Antiy-AVL&quot; &quot;Kingsoft&quot; &quot;Gridinsoft&quot; ## [46] &quot;Microsoft&quot; &quot;ViRobot&quot; &quot;ZoneAlarm&quot; ## [49] &quot;GData&quot; &quot;BitDefenderFalx&quot; &quot;AhnLab-V3&quot; ## [52] &quot;BitDefenderTheta&quot; &quot;TACHYON&quot; &quot;VBA32&quot; ## [55] &quot;Zoner&quot; &quot;Tencent&quot; &quot;Yandex&quot; ## [58] &quot;MAX&quot; &quot;MaxSecure&quot; &quot;Fortinet&quot; ## [61] &quot;Panda&quot; &quot;Sophos&quot; &quot;Elastic&quot; ## [64] &quot;AVG&quot; &quot;Cylance&quot; &quot;SentinelOne&quot; 3.2 Visualización de los datos Vamos a visualizar algunos datos principales por pares: plot(android %&gt;% select(total,size, times_submitted,harmless_votes,malicious_votes,positives)) Observamos en primera instancia que no existe mucha relación entre estas variables. Vamos a usar series temporales, al contar con fechas, para representar lo que ha ocurrido a lo largo del tiempo. Usaremos el paquete ggplo2 para estas representaciones: 3.2.1 Visualización de positivos Vamos a visualizar los positivos encontrados a lo largo del tiempo. Vamos a escoger a partir de una determinada fecha, ya que todos ellos se engloban en el mismo tiempo. android %&gt;% filter(scan_date&gt;=&quot;2021-11-03 07:33:53&quot;) %&gt;% ggplot(aes(x=scan_date,y=positives)) + geom_line(color=&quot;blue&quot;,size=1) + theme_minimal()+ geom_smooth(color=&quot;red&quot;) Podemos observar, en el suavizado realizado, que la media se encuentra en torno a 20-25 positivos por instante de tiempo, con ligeras fluctuaciones. "],["análisis-de-datos.-asociación.html", "Capítulo 4 Análisis de datos. Asociación 4.1 Reglas de asociación", " Capítulo 4 Análisis de datos. Asociación En esta sección vamos a aplicar los distintos procedimientos de análisis de datos vistos a lo largo de la asignatura. Para ello, usaremos el archivo que exportamos en capítulos anteriores: android-simplified.csv. En este primer capítulo trateremos sobre las reglas de asociación. 4.1 Reglas de asociación Para encontrar reglas de asociación, usaremos los paquetes arules y arulesViz library(arules) library(arulesViz) 4.1.1 Iteración 1 4.1.1.1 Elección de atributos Para poder aplicar el algoritmo a priori, debemos primeramente escoger los atributos que queremos que formen parte del conjunto de atributos para las reglas. android &lt;- read_csv(&quot;android-simplified.csv&quot;) # Vamos a escoger los siguientes atributos, eliminando fechas android &lt;- android %&gt;% select(size,times_submitted,harmless_votes, malicious_votes,positives,submission.submitter_country, additional_info.exiftool.FileType) 4.1.1.2 Discretizamos valores numéricos Vamos a discretizar aquellas columnas que solo contengan valores numéricos, aplicando tanto discretize como cut y orderer: summary(android$size) # informacion estadistica del tamaño para establecer los límites ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 1152 611482 2669106 5621897 4518866 178355426 android$size &lt;- ordered(cut(android$size, c(&quot;1152&quot;,&quot;611482&quot;,&quot;2669106&quot;,&quot;4518866&quot;,&quot;178355426&quot;)), labels = c(&quot;LessThan597KB&quot;,&quot;MoreThan597KBLessThan2.54MB&quot;,&quot;MoreThan2.54MBLessThan4.3MB&quot;,&quot;MoreThan4.3MB&quot;)) android$times_submitted &lt;- discretize(android$times_submitted,method=&quot;interval&quot;,breaks=4, labels = c(&quot;few&quot;,&quot;less&quot;,&quot;more&quot;,&quot;most&quot;)) android$harmless_votes &lt;- discretize(android$harmless_votes,method=&quot;interval&quot;,breaks=4, labels = c(&quot;few&quot;,&quot;less&quot;,&quot;more&quot;,&quot;most&quot;)) android$malicious_votes &lt;- discretize(android$malicious_votes,method=&quot;interval&quot;,breaks=4, labels = c(&quot;few&quot;,&quot;less&quot;,&quot;more&quot;,&quot;most&quot;)) android$positives &lt;- discretize(android$positives,method=&quot;interval&quot;,breaks=4, labels = c(&quot;few&quot;,&quot;less&quot;,&quot;more&quot;,&quot;most&quot;)) 4.1.1.3 Otros tratamientos Finalmente, eliminamos los valores NA: android &lt;- android %&gt;% na.omit() El resultado final que hemos obtenido en el dataset que usaremos para extraer las reglas es el siguiente: paged_table(android) dim(android) # Hemos obtenido una tabla de 181 virus y 7 atributos ## [1] 181 7 4.1.1.4 Extracción de reglas Vamos a usar el algoritmo a priori para extraer un conjunto inicial de reglas. Vamos a buscar aquellas que tengan una longitud mínima de 3 atributos, con ciertos valores de confianza y soporte para obtener un conjunto de reglas bueno: rules &lt;- android %&gt;% apriori(parameter = list(support=0.7,confidence=0.7,minlen=3)) ## Apriori ## ## Parameter specification: ## confidence minval smax arem aval originalSupport maxtime support minlen ## 0.7 0.1 1 none FALSE TRUE 5 0.7 3 ## maxlen target ext ## 10 rules TRUE ## ## Algorithmic control: ## filter tree heap memopt load sort verbose ## 0.1 TRUE TRUE FALSE TRUE 2 TRUE ## ## Absolute minimum support count: 126 ## ## set item appearances ...[0 item(s)] done [0.00s]. ## set transactions ...[34 item(s), 181 transaction(s)] done [0.00s]. ## sorting and recoding items ... [3 item(s)] done [0.00s]. ## creating transaction tree ... done [0.00s]. ## checking subsets of size 1 2 3 done [0.00s]. ## writing ... [3 rule(s)] done [0.00s]. ## creating S4 object ... done [0.00s]. summary(rules) ## set of 3 rules ## ## rule length distribution (lhs + rhs):sizes ## 3 ## 3 ## ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 3 3 3 3 3 3 ## ## summary of quality measures: ## support confidence coverage lift ## Min. :0.989 Min. :0.9944 Min. :0.9890 Min. :1.000 ## 1st Qu.:0.989 1st Qu.:0.9972 1st Qu.:0.9890 1st Qu.:1.003 ## Median :0.989 Median :1.0000 Median :0.9890 Median :1.006 ## Mean :0.989 Mean :0.9981 Mean :0.9908 Mean :1.004 ## 3rd Qu.:0.989 3rd Qu.:1.0000 3rd Qu.:0.9917 3rd Qu.:1.006 ## Max. :0.989 Max. :1.0000 Max. :0.9945 Max. :1.006 ## count ## Min. :179 ## 1st Qu.:179 ## Median :179 ## Mean :179 ## 3rd Qu.:179 ## Max. :179 ## ## mining info: ## data ntransactions support confidence ## . 181 0.7 0.7 ## call ## apriori(data = ., parameter = list(support = 0.7, confidence = 0.7, minlen = 3)) 4.1.1.5 Postprocessing y visualización Hemos obtenido un total de 3 reglas con longitud 3 y valores de confianza y soporte mínimos de 0.99 y 0.98, respectivamente. La media de la confianza obtenida entre todas las reglas es bastante alta, de 0.9981. Vamos a aplicar algunos filtros y extraer algunas reglas relevantes: # eliminamos redundantes rules &lt;- rules[!is.redundant(rules)] # ordenamos por lift rules_sorted &lt;- sort(rules, by=&quot;lift&quot;) plot(rules_sorted, method=&quot;graph&quot;, engine=&quot;graphviz&quot;) Vemos que hay 3 atributos implicados: El fichero tiene pocos votos de ser no dañino El fichero tiene pocos votos de ser ser malicioso El fichero se ha subido pocas veces Podemos ver que existe una relación entre los 3, con una confianza bastante grande. Vamos a buscar otras reglas escogiendo columnas diferentes. 4.1.2 Iteración 2 4.1.2.1 Elección de atributos Vamos a aplicar de nuevo el algoritmo para otro conjunto de atributos. Nos centraremos en los resultados brindados por los antivirus. android &lt;- read_csv(&quot;android-simplified.csv&quot;) # Vamos a escoger los siguientes atributos android &lt;- android %&gt;% select(submission.submitter_country, additional_info.exiftool.FileType,contains(&quot;.result&quot;)) El resultado final que hemos obtenido en el dataset que usaremos para extraer las reglas es el siguiente: paged_table(android) dim(android) # Hemos obtenido una tabla de 183 virus y 68 atributos ## [1] 183 68 4.1.2.2 Extracción de reglas Vamos a usar el algoritmo a priori para extraer un conjunto inicial de reglas. Vamos a buscar aquellas que tengan una longitud mínima de 2 atributos, con ciertos valores de confianza y soporte para obtener un conjunto de reglas bueno: rules &lt;- android %&gt;% apriori(parameter = list(support=0.5,confidence=0.5,minlen=2)) ## Apriori ## ## Parameter specification: ## confidence minval smax arem aval originalSupport maxtime support minlen ## 0.5 0.1 1 none FALSE TRUE 5 0.5 2 ## maxlen target ext ## 10 rules TRUE ## ## Algorithmic control: ## filter tree heap memopt load sort verbose ## 0.1 TRUE TRUE FALSE TRUE 2 TRUE ## ## Absolute minimum support count: 91 ## ## set item appearances ...[0 item(s)] done [0.00s]. ## set transactions ...[1498 item(s), 183 transaction(s)] done [0.00s]. ## sorting and recoding items ... [3 item(s)] done [0.00s]. ## creating transaction tree ... done [0.00s]. ## checking subsets of size 1 2 3 done [0.00s]. ## writing ... [9 rule(s)] done [0.00s]. ## creating S4 object ... done [0.00s]. summary(rules) ## set of 9 rules ## ## rule length distribution (lhs + rhs):sizes ## 2 3 ## 6 3 ## ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 2.000 2.000 2.000 2.333 3.000 3.000 ## ## summary of quality measures: ## support confidence coverage lift ## Min. :0.5191 Min. :0.6333 Min. :0.5191 Min. :1.025 ## 1st Qu.:0.5191 1st Qu.:0.8160 1st Qu.:0.5574 1st Qu.:1.136 ## Median :0.5191 Median :0.9048 Median :0.5738 Median :1.136 ## Mean :0.5398 Mean :0.8619 Mean :0.6412 Mean :1.275 ## 3rd Qu.:0.5574 3rd Qu.:0.9314 3rd Qu.:0.6831 3rd Qu.:1.464 ## Max. :0.5738 Max. :1.0000 Max. :0.8197 Max. :1.623 ## count ## Min. : 95.00 ## 1st Qu.: 95.00 ## Median : 95.00 ## Mean : 98.78 ## 3rd Qu.:102.00 ## Max. :105.00 ## ## mining info: ## data ntransactions support confidence ## . 183 0.5 0.5 ## call ## apriori(data = ., parameter = list(support = 0.5, confidence = 0.5, minlen = 2)) 4.1.2.3 Postprocessing y visualización Hemos obtenido un total de 9 reglas con longitud mínima 2 (6 reglas de longitud 2 y 3 reglas de longitud 3) y valores de confianza y soporte mínimos de 0.5. La media de la confianza obtenida entre todas las reglas es bastante alta, de 0.8619. Vamos a aplicar algunos filtros y extraer algunas reglas relevantes: # eliminamos redundantes rules &lt;- rules[!is.redundant(rules)] # ordenamos por lift rules_sorted &lt;- sort(rules, by=&quot;lift&quot;) inspect(rules_sorted@rhs) ## items ## [1] {scans.Avast-Mobile.result=Android:Evo-gen [Trj]} ## [2] {additional_info.exiftool.FileType=ZIP} ## [3] {scans.Avast-Mobile.result=Android:Evo-gen [Trj]} ## [4] {scans.Cynet.result=Malicious (score: 99)} ## [5] {scans.Avast-Mobile.result=Android:Evo-gen [Trj]} ## [6] {scans.Cynet.result=Malicious (score: 99)} ## [7] {additional_info.exiftool.FileType=ZIP} inspect(rules_sorted@lhs) ## items ## [1] {additional_info.exiftool.FileType=ZIP, ## scans.Cynet.result=Malicious (score: 99)} ## [2] {scans.Avast-Mobile.result=Android:Evo-gen [Trj]} ## [3] {additional_info.exiftool.FileType=ZIP} ## [4] {scans.Avast-Mobile.result=Android:Evo-gen [Trj]} ## [5] {scans.Cynet.result=Malicious (score: 99)} ## [6] {additional_info.exiftool.FileType=ZIP} ## [7] {scans.Cynet.result=Malicious (score: 99)} plot(rules_sorted, method=&quot;graph&quot;, engine=&quot;graphviz&quot;) Vemos que hay 3 atributos implicados: El fichero es de tipo ZIP El antivirus Avast Mobile ha dado positivo en virus Evo-gen El antivirus Cynet ha dado positivo en virus Malicious (Score:99) Los resultados de estos tres atributos parecen estar bastante enlazados, con un lift siempre mayor que 1 para todas las reglas extraídas. "],["análisis-de-datos.-clustering.html", "Capítulo 5 Análisis de datos. Clustering 5.1 Clustering", " Capítulo 5 Análisis de datos. Clustering En esta sección vamos a aplicar los distintos procedimientos de análisis de datos vistos a lo largo de la asignatura. Para ello, usaremos el archivo que exportamos en capítulos anteriores: android-simplified.csv. En este segundo capítulo trateremos sobre las clustering. 5.1 Clustering Para aplicar clustering necesitamos de valores numéricos, al contrario de lo que hemos tenido que tratar al buscar reglas de asociación. Vamos a intentar establecer grupos entre los virus subidos dependiendo de ciertos criterios. Para agrupar en clusters, usaremos los paquetes cluster y factoextra: library(cluster) library(factoextra) 5.1.1 Elección de atributos Para poder aplicar clustering, debemos primeramente escoger los atributos que queremos que formen parte del conjunto a comparar. El algoritmo kmeans se encargará de establecer los grupos android &lt;- read_csv(&quot;android-simplified.csv&quot;) # Vamos a eliminar las fechas, el país y el resultado de los escaneos android &lt;- android %&gt;% select(-c(scan_date, first_seen, last_seen, submission.submitter_country, additional_info.exiftool.FileType,contains(&quot;.result&quot;))) 5.1.2 Otros tratamientos Vamos a modificar los valores NA, que se encuentran en la columna TargetSDKVersion (Version de Android), por la media truncada del resto de valores de la columna android &lt;- android %&gt;% mutate(additional_info.androguard.TargetSdkVersion= ifelse(is.na(additional_info.androguard.TargetSdkVersion),trunc(mean(additional_info.androguard.TargetSdkVersion, na.rm=TRUE)), additional_info.androguard.TargetSdkVersion)) El resultado final que hemos obtenido en el dataset que usaremos para aplicar el algoritmo kmeans es el siguiente: paged_table(android) dim(android) # Hemos obtenido una tabla de 183 virus y 8 atributos, todos valores numéricos ## [1] 183 8 5.1.3 Agrupación en clusters. Iteración 1 Vamos a usar el algoritmo kmeans para encontrar, en una primera iteración, los grupos mejor representados. Para ello, vamos a calcular previamente cuál sería el número óptimo de clusters: android %&gt;% fviz_nbclust(FUNcluster = kmeans, method = &quot;wss&quot;, diss = dist(android, method = &quot;manhattan&quot;)) La recta se aplana cuando el numero de clusters es 5, por lo que el error cometido de usar más clusters es irrelevante. Vamos a ejecutar ahora el algoritmo kmeans para 5 clusters: grpAnd &lt;- kmeans(android, centers = 5) grpAnd ## K-means clustering with 5 clusters of sizes 79, 14, 3, 86, 1 ## ## Cluster means: ## ...1 total size times_submitted harmless_votes malicious_votes ## 1 96.83544 60.88608 4044664.1 5.189873 0.0000000 0.02531646 ## 2 83.21429 62.21429 16716171.1 1.428571 0.0000000 0.00000000 ## 3 72.33333 61.66667 74811670.0 3.666667 0.0000000 0.00000000 ## 4 89.75581 59.96512 842579.7 48.232558 0.2093023 0.15116279 ## 5 85.00000 61.00000 178355426.0 1.000000 0.0000000 0.00000000 ## positives additional_info.androguard.TargetSdkVersion ## 1 21.75949 24.00000 ## 2 17.85714 22.57143 ## 3 16.00000 27.66667 ## 4 22.23256 22.98837 ## 5 15.00000 29.00000 ## ## Clustering vector: ## [1] 4 1 1 4 4 1 1 1 1 4 1 1 2 1 4 2 1 1 4 4 4 1 4 4 4 4 4 4 1 4 1 4 1 2 1 4 1 ## [38] 1 4 1 4 4 1 4 4 1 4 1 2 3 1 4 1 4 2 4 4 2 4 1 4 4 1 3 4 1 4 1 4 1 4 4 4 1 ## [75] 2 1 1 4 1 4 1 4 4 1 5 4 4 4 4 2 4 4 4 4 1 4 4 1 4 1 4 4 3 4 2 1 1 2 4 1 1 ## [112] 1 1 4 4 4 4 4 2 4 1 4 4 4 1 1 1 1 1 1 2 4 4 1 1 1 2 1 1 4 1 4 4 1 4 4 1 1 ## [149] 4 1 4 1 1 4 4 1 1 1 1 4 1 4 4 1 4 1 1 4 1 4 1 1 1 1 2 1 4 4 1 4 4 1 4 ## ## Within cluster sum of squares by cluster: ## [1] 2.263894e+14 2.889812e+14 1.673119e+15 6.316953e+13 0.000000e+00 ## (between_SS / total_SS = 95.5 %) ## ## Available components: ## ## [1] &quot;cluster&quot; &quot;centers&quot; &quot;totss&quot; &quot;withinss&quot; &quot;tot.withinss&quot; ## [6] &quot;betweenss&quot; &quot;size&quot; &quot;iter&quot; &quot;ifault&quot; Hemos obtenido un total de 5 clusters de tamaño 79, 14, 3, 86, 1. Vamos a visualizar los virus según el cluster: fviz_cluster(grpAnd, data=android) En la grafica podemos encontrar valores muy dispersos como la fila 4, 45, 40, 70, 162, 85, 103, 50 y 64. Vamos a prescindir de ellos para encontrar grupos más cercanos entre ellos. Al eliminar estas filas, las columnas malicious_votes y harmless_votes son costantes, por lo que las vamos a eliminar. 5.1.4 Agrupación en clusters. Iteración 2 Eliminamos las columnas con datos dispares: android &lt;- android %&gt;% filter(!...1 %in% c(4, 45, 40, 70, 162, 85, 103, 50, 64)) %&gt;% select(-c(harmless_votes,malicious_votes)) Vamos a repetir el proceso de buscar el mejor numero de clusters: android %&gt;% fviz_nbclust(FUNcluster = kmeans, method = &quot;wss&quot;, diss = dist(android, method = &quot;manhattan&quot;)) En este caso, el número de clusters óptimo se ha reducido a 4. Procedamos a realizar la agrupación: grpAnd &lt;- kmeans(android, centers = 4) grpAnd ## K-means clustering with 4 clusters of sizes 77, 54, 13, 30 ## ## Cluster means: ## ...1 total size times_submitted positives ## 1 100.07792 60.87013 2543105.7 1.597403 19.98701 ## 2 82.51852 59.25926 230482.3 4.925926 23.81481 ## 3 79.07692 62.53846 17163663.7 1.384615 17.92308 ## 4 100.76667 60.80000 6043629.6 11.500000 22.83333 ## additional_info.androguard.TargetSdkVersion ## 1 23.62338 ## 2 22.87037 ## 3 22.53846 ## 4 24.46667 ## ## Clustering vector: ## [1] 1 1 1 1 1 1 1 1 2 1 1 3 4 2 3 4 4 1 2 2 1 1 2 2 2 2 2 4 2 4 2 4 3 4 2 1 4 ## [38] 1 2 2 4 2 1 1 1 3 4 2 1 1 3 2 2 3 2 1 2 2 4 2 1 1 1 1 2 1 2 1 3 4 1 2 4 2 ## [75] 4 1 2 1 2 2 2 2 3 2 1 2 2 4 2 2 1 1 1 2 2 2 3 1 1 3 1 4 4 1 1 1 1 2 2 2 3 ## [112] 2 1 2 2 1 1 1 1 1 1 1 3 1 1 1 1 1 4 1 1 1 1 2 2 4 1 2 1 1 2 4 2 4 1 1 2 4 ## [149] 1 4 1 2 4 1 1 1 4 4 1 1 2 4 1 1 1 3 4 1 2 4 1 1 4 1 ## ## Within cluster sum of squares by cluster: ## [1] 2.500585e+13 4.497025e+12 2.525358e+14 7.822480e+13 ## (between_SS / total_SS = 90.1 %) ## ## Available components: ## ## [1] &quot;cluster&quot; &quot;centers&quot; &quot;totss&quot; &quot;withinss&quot; &quot;tot.withinss&quot; ## [6] &quot;betweenss&quot; &quot;size&quot; &quot;iter&quot; &quot;ifault&quot; Hemos obtenido un total de 4 clusters de tamaño 77, 54, 30 y 13. Vamos a visualizar los virus según el cluster: fviz_cluster(grpAnd, data=android) Hemos obtenido por tanto 4 grupos bien diferenciados, que se pueden usar para buscar patrones entre ellos como versiones de Android afectadas, positivos totales Vamos a recoger toda esta información de una manera más simple presentando una tabla que relacione cada virus, con un identificador que vamos a añadirle, y el grupo en el que se encuentra. android &lt;- android %&gt;% mutate(ID = paste(&quot;Virus&quot;,seq(1:dim(android)[1]), sep=&quot;&quot;)) # Añadimos los identificadores orden &lt;- order(grpAnd$cluster) # Extraemos el orden paged_table(data.frame(android$ID[orden], grpAnd$cluster[orden])) "],["análisis-de-datos.-fca.html", "Capítulo 6 Análisis de datos. FCA 6.1 Análisis de conceptos formales 6.2 Trabajo previo 6.3 Creación del objeto formal 6.4 Búsqueda de información por clarificación y reducción 6.5 Extracción de conceptos e implicaciones", " Capítulo 6 Análisis de datos. FCA En esta sección vamos a aplicar los distintos procedimientos de análisis de datos vistos a lo largo de la asignatura. Para ello, usaremos el archivo que exportamos en capítulos anteriores: android-simplified.csv. En este segundo capítulo trateremos sobre el análisis de conceptos formales 6.1 Análisis de conceptos formales El análisis de conceptos formales es un método para el análisis de datos en cuanto a sus relaciones y estructura, organizando los datos siguiendo el pensamiento humano. Un contexto formal se define por atributos, objetos y las relaciones entre ellos, \\(K:=(G,M,I)\\). 6.2 Trabajo previo Para este proyecto vamos a tomar como objetos todos los virus que estamos analizando, como atributos la detección por un antivirus y como relación colocaremos un 0 a aquellos virus que no han sido identificados como tal por un antivirus, y un 1 si sí lo han hecho. Por tanto, debemos construir una matriz, sobre la que crearemos el contexto formal que tenga en las filas los objetos (virus) y en las columnas los atributos (resultados de cada antivirus): android &lt;- read_csv(&quot;android-simplified.csv&quot;) #Transformamos a matriz de 0 y 1 android &lt;- android %&gt;% select(contains(&quot;scans.&quot;)) %&gt;% mutate_all(~replace(., !is.na(.), 1)) %&gt;% mutate_all(~replace(., is.na(.), 0)) paged_table(android) De esta tabla que hemos creado podemos realizar algunos cálculos, cómo en cúantos antivirus un virus ha dado positivo, y en cuantos virus un antivirus ha arrojado positivo: # Calculo de positivos para el objeto n n &lt;- sample(1:dim(android)[1],1) # extraemos indice aleatorio sum(as.numeric(android[n,])) # calculamos en cuantos antivirus ha dado positivo ## [1] 17 # Calculo de positivos dados por el antivirus n &lt;- sample(1:dim(android)[2],1) # extraemos indice aleatorio sum(as.numeric(android[,n] %&gt;% pull())) # calculamos en cuantos virus ha dado positivo ## [1] 97 #Si dividimos el resultado entre el total de virus, podemos calcular el porcentaje de detección sum(as.numeric(android[,n] %&gt;% pull()))/dim(android)[1]*100 ## [1] 53.00546 Vamos a calcular el porcentaje de detección de cada antivirus df &lt;- NULL # creamos dataset #añadimos datos para cada antivirus for (n in seq(1:dim(android)[2])) { df$antivirus &lt;- rbind(df$antivirus,colnames(android[n]) %&gt;% str_replace(&quot;scans.&quot;, &quot;&quot;) %&gt;% str_replace(&quot;.result&quot;, &quot;&quot;)) df$porcentaje_deteccion &lt;- rbind(df$porcentaje_deteccion,sum(as.numeric(android[,n] %&gt;% pull()))/dim(android)[1]*100) } df &lt;- data.frame(df, stringsAsFactors = FALSE) df &lt;- df %&gt;% arrange(desc(porcentaje_deteccion)) # ordenamos por porcentaje de deteccion paged_table(df) Esta información también puede ser importante a la hora de elegir, si los virus son realmente virus, cuales de los antivirus tienen un mejor desempeño. 6.3 Creación del objeto formal Volviendo al análisis de objetos formales, usaremos el paquete fcaR: library(fcaR) Vamos a crear, en primer lugar, el objeto fca con la matriz de 0 y 1 que creamos anteriormente: #Creamos contexto formal fc &lt;- FormalContext$new(android) atributos &lt;- fc$attributes objetos &lt;- fc$objects 6.4 Búsqueda de información por clarificación y reducción En primer lugar, vamos a intentar agrupar tanto objetos como atributos, y poder extraer información de estas agrupaciones. Para ello, ejecutaremos las funciones: #Agrupamos y reducimos fc &lt;- fc$clarify(TRUE) fc &lt;- fc$reduce(TRUE) new_atributos &lt;- fc$attributes new_objetos &lt;- fc$objects Una vez aplicado reducción y clarificación, sacamos varias conclusiones: Hay antivirus que nunca han detectado nada setdiff(atributos,new_atributos) #quitando avast, avg, trendmicro-* ## [1] &quot;scans.Bkav.result&quot; &quot;scans.CMC.result&quot; ## [3] &quot;scans.ALYac.result&quot; &quot;scans.Malwarebytes.result&quot; ## [5] &quot;scans.K7AntiVirus.result&quot; &quot;scans.Arcabit.result&quot; ## [7] &quot;scans.Baidu.result&quot; &quot;scans.TrendMicro-HouseCall.result&quot; ## [9] &quot;scans.Avast.result&quot; &quot;scans.BitDefender.result&quot; ## [11] &quot;scans.SUPERAntiSpyware.result&quot; &quot;scans.MicroWorld-eScan.result&quot; ## [13] &quot;scans.Ad-Aware.result&quot; &quot;scans.TrendMicro.result&quot; ## [15] &quot;scans.FireEye.result&quot; &quot;scans.Emsisoft.result&quot; ## [17] &quot;scans.Gridinsoft.result&quot; &quot;scans.ViRobot.result&quot; ## [19] &quot;scans.BitDefenderTheta.result&quot; &quot;scans.TACHYON.result&quot; ## [21] &quot;scans.VBA32.result&quot; &quot;scans.Zoner.result&quot; ## [23] &quot;scans.Fortinet.result&quot; &quot;scans.Panda.result&quot; ## [25] &quot;scans.Elastic.result&quot; &quot;scans.AVG.result&quot; ## [27] &quot;scans.Cylance.result&quot; &quot;scans.SentinelOne.result&quot; Hay dos pares de antivirus que detectan lo mismo -&gt; mismo motor new_atributos[c(39,40)] # se han simplificado, indicando que aportan los mismos resultados ## [1] &quot;[scans.TrendMicro-HouseCall.result, scans.TrendMicro.result]&quot; ## [2] &quot;[scans.Avast.result, scans.AVG.result]&quot; android &lt;- read_csv(&quot;android-simplified.csv&quot;) paged_table(android %&gt;% select(&quot;scans.Avast.result&quot;, &quot;scans.AVG.result&quot;)) paged_table(android %&gt;% select(&quot;scans.TrendMicro-HouseCall.result&quot;, &quot;scans.TrendMicro.result&quot;)) Concluimos, por tanto, que tanto Avast y AVG, y TrendMicro-HouseCall y TrendMicro usan el mismo motor de detección Hay objetos que son exactamente iguales new_objetos[155:length(new_objetos)] ## [1] &quot;[2, 169]&quot; &quot;[5, 133]&quot; &quot;[12, 46, 148]&quot; ## [4] &quot;[13, 90, 108, 119]&quot; &quot;[22, 129, 159]&quot; &quot;[49, 131, 175]&quot; ## [7] &quot;[54, 145]&quot; &quot;[84, 100, 130, 141]&quot; &quot;[109, 165]&quot; ## [10] &quot;[113, 127]&quot; &quot;[171, 182]&quot; Los siguientes objetos parecen ser los mismos virus, al dar positivo de los mismos antivirus. 6.5 Extracción de conceptos e implicaciones En este último análisis, vamos a intentar extraer tanto conceptos como implicaciones. 6.5.1 Conceptos #Buscamos conceptos y representamos fc$find_concepts() fc$concepts[which(fc$concepts$support()&gt;0.95)] ## A set of 4 concepts: ## 1: ({1, 3, 4, 6, 7, 8, 9, 10, 11, 14, 15, 16, 17, 18, 19, 20, 21, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 47, 48, 50, 51, 52, 53, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 85, 86, 87, 88, 89, 91, 92, 93, 94, 95, 96, 97, 98, 99, 101, 102, 103, 104, 105, 106, 107, 110, 111, 112, 114, 115, 116, 117, 118, 120, 121, 122, 123, 124, 125, 126, 128, 132, 134, 135, 136, 137, 138, 139, 140, 142, 143, 144, 146, 147, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 160, 161, 162, 163, 164, 166, 167, 168, 170, 172, 173, 174, 176, 177, 178, 179, 180, 181, 183, [2, 169], [5, 133], [12, 46, 148], [13, 90, 108, 119], [22, 129, 159], [49, 131, 175], [54, 145], [84, 100, 130, 141], [109, 165], [113, 127], [171, 182]}, {}) ## 2: ({1, 3, 4, 6, 7, 8, 9, 10, 11, 14, 15, 16, 17, 18, 19, 20, 21, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 38, 39, 40, 41, 42, 43, 44, 45, 47, 48, 50, 51, 52, 53, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 80, 81, 82, 83, 85, 86, 87, 88, 89, 91, 92, 93, 94, 95, 96, 97, 98, 101, 102, 103, 104, 105, 106, 107, 110, 111, 112, 114, 115, 116, 117, 118, 120, 121, 122, 123, 124, 125, 126, 128, 132, 134, 135, 136, 137, 138, 139, 140, 142, 144, 146, 147, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 160, 161, 162, 163, 164, 166, 167, 168, 170, 172, 173, 174, 176, 177, 178, 179, 181, 183, [2, 169], [5, 133], [12, 46, 148], [13, 90, 108, 119], [22, 129, 159], [49, 131, 175], [54, 145], [84, 100, 130, 141], [109, 165], [113, 127], [171, 182]}, {scans.Ikarus.result}) ## 3: ({1, 3, 4, 6, 7, 8, 9, 10, 11, 14, 15, 16, 17, 18, 19, 20, 21, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 47, 48, 50, 51, 52, 53, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 85, 86, 87, 88, 89, 91, 92, 93, 94, 95, 96, 97, 98, 99, 101, 102, 103, 104, 105, 106, 107, 110, 111, 112, 114, 115, 116, 117, 118, 120, 121, 122, 123, 124, 125, 126, 128, 132, 134, 135, 136, 137, 138, 139, 140, 142, 143, 144, 146, 147, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 160, 161, 162, 163, 164, 166, 167, 168, 170, 172, 173, 174, 176, 177, 178, 179, 183, [2, 169], [5, 133], [12, 46, 148], [13, 90, 108, 119], [22, 129, 159], [49, 131, 175], [54, 145], [84, 100, 130, 141], [109, 165], [113, 127], [171, 182]}, {scans.ESET-NOD32.result}) ## 4: ({1, 3, 4, 6, 7, 8, 9, 10, 11, 14, 15, 16, 17, 18, 19, 20, 21, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 38, 39, 40, 41, 42, 43, 44, 45, 47, 48, 50, 51, 52, 53, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 80, 81, 82, 83, 85, 86, 87, 88, 89, 91, 92, 93, 94, 95, 96, 97, 98, 101, 102, 103, 104, 105, 106, 107, 110, 111, 112, 114, 115, 116, 117, 118, 120, 121, 122, 123, 124, 125, 126, 128, 132, 134, 135, 136, 137, 138, 139, 140, 142, 144, 146, 147, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 160, 161, 162, 163, 164, 166, 167, 168, 170, 172, 173, 174, 176, 177, 178, 179, 183, [2, 169], [5, 133], [12, 46, 148], [13, 90, 108, 119], [22, 129, 159], [49, 131, 175], [54, 145], [84, 100, 130, 141], [109, 165], [113, 127], [171, 182]}, {scans.ESET-NOD32.result, scans.Ikarus.result}) De los conceptos con más soporte podemos extraer que tanto ESET-NOD32 como Ikarus aparecen presentes en la mayoría de las detecciones 6.5.2 Implicaciones Vamos a buscar implicaciones, para posteriormente analizarlas como si se tratasen de reglas de asociación, usando para ello el paquete arules fc$find_implications() # buscamos implicaciones rules &lt;- fc$implications$to_arules() # extraemos reglas rules_subset &lt;- subset(rules, subset = support&gt;0.7) # filtramos aquellas con soporte alto length(rules_subset) # hemos encontrado un total de 6 reglas ## [1] 6 Hemos conseguido 6 reglas con soporte bastante alto. Vamos a explorarlas inspect(rules_subset) ## lhs rhs support confidence lift count addedValue boost casualConfidence casualSupport centeredConfidence certainty chiSquared collectiveStrength confirmedConfidence conviction cosine counterexample coverage doc fishersExactTest gini hyperConfidence hyperLift imbalance implicationIndex importance improvement jaccard jMeasure kappa kulczynski lambda laplace leastContradiction lerman leverage maxconfidence mutualInformation oddsRatio phi ralambondrainy relativeRisk rhsSupport RLD rulePowerFactor sebag stdLift table.n11 table.n01 table.n10 table.n00 varyingLiaison yuleQ yuleY ## [1] {scans.AhnLab-V3.result} =&gt; {scans.ESET-NOD32.result} 0.8666667 1 1.012270 143 0.01212121 Inf 1 1.854545 0.01212121 1 13.159509 2.373017 1 Inf 0.9366433 1 0.8666667 0.09090909 1.707317e-02 0.0019100092 0.9829268 1.000000 0.12269939 -1.316561 0.05498644 1 0.8773006 NaN 0.14772727 0.9386503 0.0000000 0.9931034 0.8773006 0.1458352 0.010505051 1 NaN Inf 0.2824086 0 1.100000 0.9878788 1 0.8666667 Inf 1 143 20 0 2 0.01226994 NaN NaN ## [2] {scans.DrWeb.result} =&gt; {scans.ESET-NOD32.result} 0.9030303 1 1.012270 149 0.01212121 Inf 1 1.890909 0.01212121 1 18.853528 2.545002 1 Inf 0.9560912 1 0.9030303 0.12500000 8.869180e-03 0.0027364555 0.9911308 1.006757 0.08588957 -1.343898 0.07629556 1 0.9141104 NaN 0.20509291 0.9570552 0.0000000 0.9933775 0.9141104 0.1488632 0.010945822 1 NaN Inf 0.3380293 0 1.142857 0.9878788 1 0.9030303 Inf 1 149 14 0 2 0.01226994 NaN NaN ## [3] {scans.Kaspersky.result, ## scans.Ikarus.result} =&gt; {scans.ESET-NOD32.result} 0.8000000 1 1.012270 132 0.01212121 Inf 1 1.787879 0.01212121 1 8.098160 2.229897 1 Inf 0.8998977 1 0.8000000 0.06060606 3.902439e-02 0.0011753903 0.9609756 1.000000 0.19018405 -1.264911 0.03566491 1 0.8098160 NaN 0.09356725 0.9049080 0.0000000 0.9925373 0.8098160 0.1401139 0.009696970 1 NaN Inf 0.2215395 0 1.064516 0.9878788 1 0.8000000 Inf 1 132 31 0 2 0.01226994 NaN NaN ## [4] {scans.Kaspersky.result, ## scans.McAfee-GW-Edition.result} =&gt; {scans.ESET-NOD32.result} 0.7030303 1 1.012270 116 0.01212121 Inf 1 1.690909 0.01212121 1 4.792788 2.139901 1 Inf 0.8435973 1 0.7030303 0.04081633 8.691796e-02 0.0006956391 0.9130820 1.000000 0.28834356 -1.185774 0.02263279 1 0.7116564 NaN 0.05645456 0.8558282 0.0000000 0.9915254 0.7116564 0.1313479 0.008521579 1 NaN Inf 0.1704324 0 1.042553 0.9878788 1 0.7030303 Inf 1 116 47 0 2 0.01226994 NaN NaN ## [5] {scans.Cynet.result} =&gt; {scans.Avira.result} 0.8000000 1 1.130137 132 0.11515152 Inf 1 1.684848 0.11515152 1 85.890411 67.915575 1 Inf 0.9508468 1 0.8000000 0.57575758 2.157753e-16 0.1060789715 1.0000000 1.090909 0.09589041 -3.898718 0.36472363 1 0.9041096 NaN 0.68468468 0.9520548 0.2631579 0.9925373 0.9041096 1.4064431 0.092121212 1 NaN Inf 0.7214901 0 2.357143 0.8848485 1 0.8000000 Inf 1 132 14 0 19 0.13013699 NaN NaN ## [6] {scans.McAfee.result, ## scans.K7GW.result, ## scans.ESET-NOD32.result, ## scans.AhnLab-V3.result} =&gt; {scans.Ikarus.result} 0.7151515 1 1.031250 118 0.03030303 Inf 1 1.684848 0.03030303 1 12.945479 6.017652 1 Inf 0.8587782 1 0.7151515 0.10638298 1.600048e-03 0.0046109060 0.9984000 1.008547 0.26250000 -1.890967 0.05309334 1 0.7375000 NaN 0.14549938 0.8687500 0.0000000 0.9916667 0.7375000 0.3342790 0.021671258 1 NaN Inf 0.2801026 0 1.119048 0.9696970 1 0.7151515 Inf 1 118 42 0 5 0.03125000 NaN NaN De estas reglas, podemos extraer información viendo qué antivirus han arrojado un valor positivo siempre que otro también lo han hecho, así se podría predecir el comportamiento de los presentes. "],["análisis-de-datos.-regresion.html", "Capítulo 7 Análisis de datos. Regresion 7.1 Regresión", " Capítulo 7 Análisis de datos. Regresion En esta sección vamos a aplicar los distintos procedimientos de análisis de datos vistos a lo largo de la asignatura. Para ello, usaremos el archivo que exportamos en capítulos anteriores: android-simplified.csv. En este segundo capítulo trateremos sobre técnicas de regresión univariables, multivariables y comparación de modelos. 7.1 Regresión Vamos a intentar en primer lugar, haciendo uso de la orden lm, una regresión univariable para intentar predecir el número de positivos que arroja cada virus, en función del resto de las variables numéricas. Como vimos en el capítulo de visualización inicial, vamos a filtrar para evitar aquellos valores anómalos. dataset &lt;- read.csv(&quot;android-simplified.csv&quot;) model &lt;- lm( positives ~ ., data = dataset %&gt;% filter(scan_date&gt;=&quot;2021-11-03 07:33:53&quot;) %&gt;% select(total,size, times_submitted,harmless_votes,malicious_votes,positives)) summary(model) ## ## Call: ## lm(formula = positives ~ ., data = dataset %&gt;% filter(scan_date &gt;= ## &quot;2021-11-03 07:33:53&quot;) %&gt;% select(total, size, times_submitted, ## harmless_votes, malicious_votes, positives)) ## ## Residuals: ## Min 1Q Median 3Q Max ## -7.4632 -3.6601 -0.4453 3.1708 13.5061 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 7.631e+00 1.207e+01 0.632 0.528187 ## total 2.412e-01 1.995e-01 1.209 0.228687 ## size -7.077e-08 2.078e-08 -3.406 0.000861 *** ## times_submitted -1.827e-03 1.302e-03 -1.403 0.162705 ## harmless_votes -5.249e+00 2.394e+00 -2.193 0.029958 * ## malicious_votes 7.533e+00 3.306e+00 2.279 0.024185 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.591 on 140 degrees of freedom ## Multiple R-squared: 0.1299, Adjusted R-squared: 0.09884 ## F-statistic: 4.181 on 5 and 140 DF, p-value: 0.00143 Vemos que el resultado del model es bastante malo, la R2 es de 0.09884, y únicamente la variable size ha arrojado un valor significativo para incluirla, en el resto no podemos aceptar la hipótesis nula. Vamos a representar los valores de los positivos a lo largo del tiempo: android %&gt;% filter(scan_date&gt;=&quot;2021-11-03 07:33:53&quot;) %&gt;% ggplot(aes(x=scan_date,y=positives)) + geom_point(color=&quot;blue&quot;,size=1) + theme_minimal() + geom_line(aes(x=scan_date,y=predict(model,android %&gt;% filter(scan_date&gt;=&quot;2021-11-03 07:33:53&quot;)),color=&quot;red&quot;)) Vamos a intentar realizar regresión con otras variables. model &lt;- lm( harmless_votes ~ malicious_votes, data = dataset) summary(model) ## ## Call: ## lm(formula = harmless_votes ~ malicious_votes, data = dataset) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.3557 0.0139 0.0139 0.0139 0.2086 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.01390 0.01068 -1.301 0.195 ## malicious_votes 1.36964 0.01105 123.911 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.144 on 181 degrees of freedom ## Multiple R-squared: 0.9883, Adjusted R-squared: 0.9883 ## F-statistic: 1.535e+04 on 1 and 181 DF, p-value: &lt; 2.2e-16 Malicious_votes está muy relacionada con Harmless_votes y un poco con positives. No es fiable porque solo representa 3 casos aislados y todos los que son 0. Osea, cuando no hay harmless_votes no suele haber malicious_votes, algo de sentido común si los archivos que se suben no tienen nada raro. Observando el resto de variables, no vamos a obtener nada en claro, y parece que no vamos a poder encontrar ninguna regresión lo suficientemente buena para incluirla como conclusión final. "],["conclusiones.html", "Capítulo 8 Conclusiones", " Capítulo 8 Conclusiones Como conclusión a este proyecto, podemos recalcar que el análisis de datos es un área enorme de la informática. Hemos extraído información de numerosas maneras, a pesar de que el dataset no es tan extenso como las compañias suelen tratar mediante Big Data. Hemos aprendido a construir un dataset desde cero, y hemos aplicado preprocessing y filtrado en cada nueva herramienta de análisis de datos que hemos tenido que usar. Esta fase del análisis es la más importante, y es la que menos suelen pensar al iniciarse en esta disciplina. Una de las técnicas más relevantes y la que nos ha ofrecido mejores resultados es el análisis de conceptos formales. Incluso ha servido para poder extraer reglas fácilmente y poder ser tratadas como reglas de asociación, con implicaciones mucho mejor desarrolladas y significativas. En definitiva, trabajar con este proyecto nos ha servido para involucrarnos en una tarea real de análisis de datos, y esperemos que no sea la última, porque esta disciplina es una de nuestras favoritas. "],["text-mining-en-app-de-mensajería.html", "Capítulo 9 Text Mining en app de mensajería", " Capítulo 9 Text Mining en app de mensajería All chapters start with a first-level heading followed by your chapter title, like the line above. There should be only one first-level heading (#) per .Rmd file. "],["referencias.html", "Referencias", " Referencias "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
